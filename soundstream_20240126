# Copyright (C) 2023 Katsuya Iida. All rights reserved.
import os
import wave
from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d
from typing import Tuple, List, Optional
import torch
from scipy.io.wavfile import read
from torch.nn.utils import weight_norm,spectral_norm
from utils import init_weights, get_padding
import torchvision
import argparse
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
from models import Generator, MultiPeriodDiscriminator, MultiScaleDiscriminator, feature_loss, generator_loss,\
    discriminator_loss
try:
    import pytorch_lightning as pl
except ImportError:
    class pl:
        class LightningModule:
            pass

        class Callback:
            pass
from itertools import chain
import random

LRELU_SLOPE = 0.1

def load_wav(full_path):
    sampling_rate, data = read(full_path)
    return data, sampling_rate

class ResNet1d(nn.Module):
    def __init__(
        self,
        n_channels,
        kernel_size: int = 7,
        padding: str = 'valid',
        dilation: int = 1
    ) -> None:
        super().__init__()
        assert padding in ['valid', 'same']
        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation = dilation
        self._padding_size = (kernel_size // 2) * dilation
        self.conv0 = nn.Conv1d(
            n_channels,
            n_channels,
            kernel_size=kernel_size,
            padding=padding,
            dilation=dilation)
        self.conv1 = nn.Conv1d(
            n_channels,
            n_channels,
            kernel_size=1)

    def forward(self, input):
        y = input
        x = self.conv0(input)
        x = F.elu(x)
        x = self.conv1(x)
        if self.padding == 'valid':
            y = y[:, :, self._padding_size:-self._padding_size]
        x += y
        x = F.elu(x)
        return x


class ResNet2d(nn.Module):
    def __init__(
        self,
        n_channels: int,
        factor: int,
        stride: Tuple[int, int]
    ) -> None:
        # https://arxiv.org/pdf/2005.00341.pdf
        # The original paper uses layer normalization, but here
        # we use batch normalization.
        super().__init__()
        self.conv0 = nn.Conv2d(
            n_channels,
            n_channels,
            kernel_size=(3, 3),
            padding='same')
        self.bn0 = nn.BatchNorm2d(
            n_channels
        )
        self.conv1 = nn.Conv2d(
            n_channels,
            factor * n_channels,
            kernel_size=(stride[0] + 2, stride[1] + 2),
            stride=stride)
        self.bn1 = nn.BatchNorm2d(
            factor * n_channels
        )
        self.conv2 = nn.Conv2d(
            n_channels,
            factor * n_channels,
            kernel_size=1,
            stride=stride)
        self.bn2 = nn.BatchNorm2d(
            factor * n_channels
        )
        self.pad = nn.ReflectionPad2d([
            (stride[1] + 1) // 2,
            (stride[1] + 2) // 2,
            (stride[0] + 1) // 2,
            (stride[0] + 2) // 2,
        ])
        self.activation = nn.LeakyReLU(0.3)

    def forward(self, input):
        x = self.conv0(input)
        x = self.bn0(x)
        x = self.activation(x)
        x = self.pad(x)
        x = self.conv1(x)
        x = self.bn1(x)

        # shortcut
        y = self.conv2(input)
        y = self.bn2(y)

        x += y
        x = self.activation(x)
        return x


class EncoderBlock(nn.Module):
    def __init__(
        self,
        n_channels: int,
        padding: str,
        stride: int
    ) -> None:
        super().__init__()
        assert padding in ['valid', 'same']
        self.layers = nn.Sequential(
            ResNet1d(n_channels // 2, padding=padding, dilation=1),
            ResNet1d(n_channels // 2, padding=padding, dilation=3),
            ResNet1d(n_channels // 2, padding=padding, dilation=9),
            nn.Conv1d(
                n_channels // 2, n_channels,
                kernel_size=2 * stride,
                padding=(2 * stride) // 2 if padding == 'same' else 0,
                stride=stride),
            nn.ELU(),
        )

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return self.layers(input)


class DecoderBlock(nn.Module):
    def __init__(
        self,
        n_channels: int,
        padding: str,
        stride: int
    ) -> None:
        super().__init__()
        assert padding in ['valid', 'same']
        self.layers = nn.Sequential(
            nn.ConvTranspose1d(
                n_channels, n_channels // 2,
                kernel_size=2 * stride,
                padding=(2 * stride) // 2 if padding == 'same' else 0,
                stride=stride),
            nn.ELU(),
            ResNet1d(n_channels // 2, padding=padding, dilation=1),
            ResNet1d(n_channels // 2, padding=padding, dilation=3),
            ResNet1d(n_channels // 2, padding=padding, dilation=9),
        )

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return self.layers(input)


class Encoder(nn.Module):
    def __init__(self, n_channels: int, padding):
        super().__init__()
        assert padding in ['valid', 'same']
        self.layers = nn.Sequential(
            nn.Conv1d(1, n_channels, kernel_size=7, padding=padding),
            nn.ELU(),
            EncoderBlock(2 * n_channels, padding=padding, stride=2),
            EncoderBlock(4 * n_channels, padding=padding, stride=4),
            EncoderBlock(8 * n_channels, padding=padding, stride=5),
            # EncoderBlock(16 * n_channels, padding=padding, stride=7),
            EncoderBlock(16 * n_channels, padding=padding, stride=6),
            nn.Conv1d(16 * n_channels, 16 * n_channels, kernel_size=3, padding=padding),
        )

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return self.layers(input)


class Decoder(nn.Module):
    def __init__(self, n_channels: int, padding):
        super().__init__()
        assert padding in ['valid', 'same']
        self.layers = nn.Sequential(
            nn.Conv1d(16 * n_channels, 16 * n_channels, kernel_size=3, padding=padding),
            nn.ELU(),
            DecoderBlock(16 * n_channels, padding=padding, stride=6),
            # DecoderBlock(16 * n_channels, padding=padding, stride=7),
            DecoderBlock(8 * n_channels, padding=padding, stride=5),
            DecoderBlock(4 * n_channels, padding=padding, stride=4),
            DecoderBlock(2 * n_channels, padding=padding, stride=2),
            nn.Conv1d(n_channels, 1, kernel_size=7, padding=padding),
            nn.Tanh(),
        )

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return self.layers(input)


class ResidualVectorQuantizer(nn.Module):
    weight: torch.Tensor
    running_mean: torch.Tensor
    code_count: torch.Tensor

    def __init__(
        self,
        num_quantizers: int,
        num_embeddings: int,
        embedding_dim: int,
        decay: float = 0.999,#张怀峰从0.99改为0.999
        code_replace_threshold: float = 0.0001,
        eps: float = 1e-10,
    ) -> None:
        super().__init__()
        self.num_quantizers = num_quantizers
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.register_buffer("weight", torch.empty(num_quantizers, num_embeddings, embedding_dim))
        self.register_buffer("running_mean", torch.empty(num_quantizers, num_embeddings, embedding_dim))
        self.register_buffer("code_count", torch.empty(num_quantizers, num_embeddings))
        self.decay = decay
        self.eps = eps
        self.code_replace_threshold = code_replace_threshold
        self.reset_parameters()

    def reset_parameters(self) -> None:
        init.uniform_(self.weight)
        self.running_mean[:] = self.weight
        init.ones_(self.code_count)

    @torch.cuda.amp.autocast(enabled=False)
    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        # input: [..., chennel]
        if self.training:
            # Enabling bitrate scalability with quantizer dropout
            n = random.randrange(1, self.num_quantizers)
        else:
            n = self.num_quantizers
        codes = []
        r = input.type_as(self.running_mean).detach()
        with torch.no_grad():
            for i in range(n):
                w = self.weight[i]
                # r: [..., num_embeddings]
                dist = torch.cdist(r, w)
                k = torch.argmin(dist, axis=-1)
                codes.append(k)
                self._update_averages(i, r, k)
                r = r - F.embedding(k, w)
        quantized = input - r
        commitment_loss = torch.mean(torch.square(input - quantized.detach()))
        self.weight.data[:] = self.running_mean / torch.unsqueeze(self.eps + self.code_count, axis=-1)
        return quantized, torch.stack(codes, input.ndim - 1), commitment_loss

    def dequantize(self, input: torch.Tensor, n: Optional[int] = None) -> torch.Tensor:
        # input: [batch_size, length, num_quantizers]
        if n is None:
            n = input.shape[-1]
        assert 0 < n <= self.num_quantizers
        res = 0
        with torch.no_grad():
            for i in range(n):
                k = input[:, :, i]
                w = self.weight[i]
                res += F.embedding(k, w)
        return res

    def _update_averages(self, i: int, r: torch.Tensor, k: torch.Tensor) -> None:
        # https://arxiv.org/pdf/1906.00446.pdf
        # Generating Diverse High-Fidelity Images with VQ-VAE-2
        # 2.1 Vector Quantized Variational AutoEncode

        # k: [...]
        one_hot_k = F.one_hot(torch.flatten(k), self.num_embeddings).type_as(self.code_count)
        code_count_update = torch.mean(one_hot_k, axis=0)
        self.code_count[i].lerp_(code_count_update, 1 - self.decay)

        # r: [..., embedding_dim]
        r = r.reshape(-1, self.embedding_dim)
        running_mean_update = (one_hot_k.T @ r) / r.shape[0]
        self.running_mean[i].lerp_(running_mean_update, 1 - self.decay)

    @torch.no_grad()
    @torch.cuda.amp.autocast(enabled=False)
    def replace_vectors(self) -> int:
        # https://arxiv.org/pdf/2107.03312.pdf
        # C. Residual Vector Quantizer:

        # The original paper replaces with an input frame randomly
        # sampled within the current batch.
        # Here we replace with random average of running mean instead.
        num_replaced = torch.sum(self.code_count < self.code_replace_threshold).item()
        if num_replaced > 0:
            for i in range(self.num_quantizers):
                mask = self.code_count[i] < self.code_replace_threshold
                # mask: [num_quantizers, num_embeddings]
                w = torch.rand_like(self.code_count[i])
                w /= torch.sum(w)
                self.running_mean[i, mask] = w.type_as(self.running_mean) @ self.running_mean[i]
                self.code_count[i, mask] = w.type_as(self.code_count) @ self.code_count[i]

        return num_replaced

    @torch.no_grad()
    def calc_entropy(self) -> float:
        p = self.code_count / (self.eps + torch.sum(self.code_count, axis=-1, keepdim=True))
        return -torch.sum(torch.log(p) * p).item() / self.num_quantizers


class WaveDiscriminator(nn.Module):
    r"""MelGAN discriminator from https://arxiv.org/pdf/1910.06711.pdf
    """
    def __init__(self, resolution: int = 1, n_channels: int = 4) -> None:
        super().__init__()
        assert resolution >= 1
        if resolution == 1:
            self.avg_pool = nn.Identity()
        else:
            self.avg_pool = nn.AvgPool1d(resolution * 2, stride=resolution)
        self.activation = nn.LeakyReLU(0.2, inplace=True)
        self.layers = nn.ModuleList([
            nn.utils.weight_norm(nn.Conv1d(1, n_channels, kernel_size=15, padding=7)),
            nn.utils.weight_norm(nn.Conv1d(n_channels, 4 * n_channels, kernel_size=41, stride=4, padding=20, groups=4)),
            nn.utils.weight_norm(nn.Conv1d(4 * n_channels, 16 * n_channels, kernel_size=41, stride=4, padding=20, groups=16)),
            nn.utils.weight_norm(nn.Conv1d(16 * n_channels, 64 * n_channels, kernel_size=41, stride=4, padding=20, groups=64)),
            nn.utils.weight_norm(nn.Conv1d(64 * n_channels, 256 * n_channels, kernel_size=41, stride=4, padding=20, groups=256)),
            nn.utils.weight_norm(nn.Conv1d(256 * n_channels, 256 * n_channels, kernel_size=5, padding=2)),
            nn.utils.weight_norm(nn.Conv1d(256 * n_channels, 1, kernel_size=3, padding=1)),
        ])

    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        x = self.avg_pool(x)
        feats = []
        for layer in self.layers[:-1]:
            x = layer(x)
            feats.append(x)
            x = self.activation(x)
        feats.append(self.layers[-1](x))
        return feats


class STFTDiscriminator(nn.Module):
    r"""STFT-based discriminator from https://arxiv.org/pdf/2107.03312.pdf
    """
    def __init__(
        self, n_fft: int = 1024, hop_length: int = 256,
        n_channels: int = 32
    ) -> None:
        super().__init__()
        self.n_fft = n_fft
        self.hop_length = hop_length
        n = n_fft // 2 + 1
        for _ in range(6):
            n = (n - 1) // 2 + 1
        self.layers = nn.Sequential(
            nn.Conv2d(1, n_channels, kernel_size=7, padding='same'),
            nn.LeakyReLU(0.3, inplace=True),
            ResNet2d(n_channels, 2, stride=(2, 1)),
            ResNet2d(2 * n_channels, 2, stride=(2, 2)),
            ResNet2d(4 * n_channels, 1, stride=(2, 1)),
            ResNet2d(4 * n_channels, 2, stride=(2, 2)),
            ResNet2d(8 * n_channels, 1, stride=(2, 1)),
            ResNet2d(8 * n_channels, 2, stride=(2, 2)),
            nn.Conv2d(16 * n_channels, 1, kernel_size=(n, 1))
        )

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        assert input.shape[1] == 1
        # input: [batch, channel, sequence]
        x = torch.squeeze(input, 1).to(torch.float32)  # torch.stft() doesn't accept float16
        x = torch.stft(x, self.n_fft, self.hop_length, normalized=True, onesided=True, return_complex=True)
        x = torch.abs(x)
        x = torch.unsqueeze(x, dim=1)
        x = self.layers(x)
        return x


class ReconstructionLoss(nn.Module):
    """Reconstruction loss from https://arxiv.org/pdf/2107.03312.pdf
    but uses STFT instead of mel-spectrogram
    """
    def __init__(self, eps=1e-5):
        super().__init__()
        self.eps = eps

    def forward(self, input, target):
        loss = 0
        input = input.to(torch.float32)
        target = target.to(torch.float32)
        for i in range(6, 12):
            s = 2 ** i
            alpha = (s / 2) ** 0.5
            # We use STFT instead of 64-bin mel-spectrogram as n_fft=64 is too small
            # for 64 bins.
            x = torch.stft(input, n_fft=s, hop_length=s // 4, win_length=s, normalized=True, onesided=True, return_complex=True)
            x = torch.abs(x)
            y = torch.stft(target, n_fft=s, hop_length=s // 4, win_length=s, normalized=True, onesided=True, return_complex=True)
            y = torch.abs(y)
            if x.shape[-1] > y.shape[-1]:
                x = x[:, :, :y.shape[-1]]
            elif x.shape[-1] < y.shape[-1]:
                y = y[:, :, :x.shape[-1]]
            loss += torch.mean(torch.abs(x - y))
            loss += alpha * torch.mean(torch.square(torch.log(x + self.eps) - torch.log(y + self.eps)))
        return loss / (12 - 6)


class ReconstructionLoss2(nn.Module):
    """Reconstruction loss from https://arxiv.org/pdf/2107.03312.pdf
    """
    def __init__(self, sample_rate, eps=1e-5):
        super().__init__()
        import torchaudio
        self.layers = nn.ModuleList()
        self.alpha = []
        self.eps = eps
        for i in range(6, 12):
            melspec = torchaudio.transforms.MelSpectrogram(
                sample_rate=sample_rate,
                n_fft=int(2 ** i),
                win_length=int(2 ** i),
                hop_length=int(2 ** i / 4),
                n_mels=64)
            self.layers.append(melspec)
            self.alpha.append((2 ** i / 2) ** 0.5)

    def forward(self, input, target):
        loss = 0
        for alpha, melspec in zip(self.alpha, self.layers):
            x = melspec(input)
            y = melspec(target)
            if x.shape[-1] > y.shape[-1]:
                x = x[:, y.shape[-1]]
            elif x.shape[-1] < y.shape[-1]:
                y = y[:, x.shape[-1]]
            loss += torch.mean(torch.abs(x - y))
            loss += alpha * torch.mean(torch.square(torch.log(x + self.eps) - torch.log(y + self.eps)))
        return loss
#####################↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓张怀峰新增##############################################



#################################上面↑↑↑↑↑↑↑↑↑↑↑↑↑，张怀峰添加################

class StreamableModel(pl.LightningModule):
    def __init__(
        self,
        n_channels: int = 32,
        num_quantizers: int = 9,
        num_embeddings: int = 1024,
        padding: str = "same",
        batch_size: int = 32,
        sample_rate: int = 8000,
        segment_length: int = 35710,
        g_lr: float = 1e-4,
        g_b1: float = 0.5,
        g_b2: float = 0.9,
        d_lr: float = 1e-5,#张怀峰增加
        d_b1: float = 0.8,#张怀峰增加
        d_b2: float = 0.99,#张怀峰增加
        dataset: str = 'librispeech'
    ) -> None:
        # https://arxiv.org/pdf/2009.02095.pdf
        # 2. Method
        # SEANet uses Adam with lr=1e-4, beta1=0.5, beta2=0.9
        # batch_size=16
        super().__init__()
        self.segment_length = segment_length
        self.save_hyperparameters()
        self.automatic_optimization = False
        self.encoder = Encoder(n_channels, padding)
        self.decoder = Decoder(n_channels, padding)
        self.quantizer = ResidualVectorQuantizer(
            num_quantizers, num_embeddings, n_channels * 16)

        # self.wave_discriminators = nn.ModuleList([
        #     WaveDiscriminator(resolution=1),
        #     WaveDiscriminator(resolution=2),
        #     WaveDiscriminator(resolution=4)
        # ])
        self.rec_loss = ReconstructionLoss()
        self.stft_discriminator = STFTDiscriminator()
        self.msd = MultiScaleDiscriminator()#张怀峰添加
        self.mpd = MultiPeriodDiscriminator()#张怀峰添加

    def configure_optimizers(self):
        g_lr = self.hparams.g_lr
        g_b1 = self.hparams.g_b1
        g_b2 = self.hparams.g_b2
        d_lr = self.hparams.d_lr#张怀峰添加
        d_b1 = self.hparams.d_b1#张怀峰添加
        d_b2 = self.hparams.d_b2#张怀峰添加

        optimizer_g = torch.optim.Adam(chain(self.encoder.parameters(),self.decoder.parameters()),lr=g_lr, betas=(g_b1, g_b2))
        # optimizer_d = torch.optim.Adam(
        #     chain(
        #         self.wave_discriminators.parameters(),
        #         self.stft_discriminator.parameters()
        #     ),
        #     lr=lr, betas=(b1, b2))
        ##张怀峰更换
        optimizer_d = torch.optim.Adam(chain(self.mpd.parameters(),self.msd.parameters()),lr=d_lr, betas=(d_b1, d_b2))
        return [optimizer_g, optimizer_d], []

    def forward(self, input):
        x = self.encoder(input)
        x = torch.transpose(x, -1, -2)
        x, codes, codebook_loss = self.quantizer(x)
        x = torch.transpose(x, -1, -2)
        x = self.decoder(x)
        return x, codes, codebook_loss

    # def training_step(self, batch, batch_idx):
    #     optimizer_g, optimizer_d = self.optimizers()
    #     input = batch[:, None, :]
    #     # input: [batch, channel, sequence]
    #
    #     # train generator
    #     self.toggle_optimizer(optimizer_g)
    #     output, _, q_loss = self.forward(input)
    #     # output: [batch, channel, sequence]
    #     # print(input.shape, output.shape)
    #
    #     stft_out = self.stft_discriminator(output)
    #     g_stft_loss = torch.mean(torch.relu(1 - stft_out))
    #     self.log("g_stft_loss", g_stft_loss, prog_bar=True)
    #
    #     g_wave_loss = 0
    #     g_feat_loss = 0
    #     for i in range(3):
    #         feats1 = self.wave_discriminators[i](input)
    #         feats2 = self.wave_discriminators[i](output)
    #         assert len(feats1) == len(feats2)
    #         g_wave_loss += torch.mean(torch.relu(1 - feats2[-1]))
    #         g_feat_loss += sum(torch.mean(
    #             torch.abs(f1 - f2))
    #             for f1, f2 in zip(feats1[:-1], feats2[:-1])) / (len(feats1) - 1)
    #     self.log("g_wave_loss", g_wave_loss / 3,prog_bar=True)
    #     self.log("g_feat_loss", g_feat_loss / 3,prog_bar=True)
    #
    #     g_rec_loss = self.rec_loss(output[:, 0, :], input[:, 0, :])
    #     self.log("g_rec_loss", g_rec_loss, prog_bar=True)
    #
    #     g_feat_loss = g_feat_loss / 3
    #     g_adv_loss = (g_stft_loss + g_wave_loss) / 4
    #     g_loss = g_adv_loss + 100 * g_feat_loss + g_rec_loss
    #     self.log("q_loss", q_loss, prog_bar=True)
    #     self.log("g_loss", g_loss, prog_bar=True)
    #
    #     self.manual_backward(g_loss + q_loss)
    #     optimizer_g.step()
    #     optimizer_g.zero_grad()
    #     self.untoggle_optimizer(optimizer_g)
    #
    #     codes_entropy = self.quantizer.calc_entropy()
    #     self.log("codes_entropy", codes_entropy, prog_bar=True)
    #
    #     # train discriminator
    #     self.toggle_optimizer(optimizer_d)
    #     output, _, _ = self.forward(input)
    #
    #     stft_out = self.stft_discriminator(input)
    #     stft_out = 1 - stft_out
    #     stft_out = torch.relu(stft_out)
    #     d_stft_loss = torch.mean(stft_out)
    #     stft_out = self.stft_discriminator(output)
    #     stft_out = 1 + stft_out
    #     stft_out = torch.relu(stft_out)
    #     d_stft_loss += torch.mean(stft_out)
    #
    #     d_wave_loss = 0
    #     for i in range(3):
    #         feats = self.wave_discriminators[i](input)
    #         feat = feats[-1]
    #         d_wave_loss += torch.mean(torch.relu(1 - feats[-1]))#feats为list7，分别是tensor:(32,4,,35710)(32,16,8928)...[-1]代表list的最后一个元素
    #         feats = self.wave_discriminators[i](output)
    #         d_wave_loss += torch.mean(torch.relu(1 + feats[-1]))
    #
    #     d_loss = (d_stft_loss + d_wave_loss) / 4
    #
    #     self.log("d_stft_loss", d_stft_loss,prog_bar=True)
    #     self.log("d_wave_loss", d_wave_loss / 3,prog_bar=True)
    #
    #     d_loss = (d_stft_loss + d_wave_loss) / 4
    #     self.log("d_loss", d_loss, prog_bar=True)
    #
    #     self.manual_backward(d_loss)
    #     optimizer_d.step()
    #     optimizer_d.zero_grad()
    #     self.untoggle_optimizer(optimizer_d)
    #
    #     num_replaced = self.quantizer.replace_vectors()
    #     self.log("num_replaced", float(num_replaced), prog_bar=True)

#↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓更换如下↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
    def training_step(self, batch, batch_idx):
        optimizer_g, optimizer_d = self.optimizers()
        input = batch[:, None, :]
        # input: [batch, channel, sequence]

        # train generator
        self.toggle_optimizer(optimizer_g)
        output, _, q_loss = self.forward(input)
        # device = torch.device('cuda:{:d}'.format(0))
        # input = torch.autograd.Variable(input.to(device, non_blocking=True))
        # output = torch.autograd.Variable(output.to(device, non_blocking=True))
        # output: [batch, channel, sequence]
        # print(input.shape, output.shape)

        stft_out = self.stft_discriminator(output)#output=(32,1,35710)
        g_stft_loss = torch.mean(torch.relu(1 - stft_out))#stft_out=(32,1,1,18)
        self.log("g_stft_loss", g_stft_loss, prog_bar=True)

        ###张怀峰替换############
        y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = self.mpd(input, output)
        y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.msd(input, output)
        # loss_mel = F.l1_loss(y_mel, y_g_hat_mel) * 45

        loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g)
        loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g)
        loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)
        loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)
        loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g)
        loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)
        # loss_gen_all = loss_gen_s + loss_gen_f + loss_fm_s + loss_fm_f + loss_mel
        g_stft_loss_mpd = 0
        g_stft_loss_msd = 0

        ############取消MPD和MSD的STFT
        # for i in range(4):
        #     g_stft_loss_mpd += torch.mean(torch.relu(1 - y_df_hat_g[i]))
        # for i in range(2):
        #     g_stft_loss_msd += torch.mean(torch.relu(1 - y_ds_hat_g[i]))
        # g_stft_loss = g_stft_loss_mpd+g_stft_loss_msd

        self.log("g_stft_loss", g_stft_loss, prog_bar=True)

        g_wave_loss = 0
        g_feat_loss = 0

        # for i in range(3):
        #     feats1 = self.wave_discriminators[i](input)
        #     feats2 = self.wave_discriminators[i](output)
        #     assert len(feats1) == len(feats2)
        #     g_wave_loss += torch.mean(torch.relu(1 - feats2[-1]))
        #     g_feat_loss += sum(torch.mean(
        #         torch.abs(f1 - f2))
        #                        for f1, f2 in zip(feats1[:-1], feats2[:-1])) / (len(feats1) - 1)
        # self.log("g_wave_loss", g_wave_loss / 3, prog_bar=True)
        # self.log("g_feat_loss", g_feat_loss / 3, prog_bar=True)

        for i in range(4):#5组
            feats1 = fmap_f_r[i]
            feats2 = fmap_f_g[i]
            # for j in range(5):
            #     feats1 = feats1_f_r[j]
            #     feats2 = feats2_f_g[j]
            assert len(feats1) == len(feats2)
            feats2_last = feats2[-1]
            feats2_last = feats2_last.narrow(3,1,1)
            feats2_last = torch.squeeze(feats2_last,3)
            a = len(feats1)
            g_wave_loss += torch.mean(torch.relu(1 - feats2_last))
            g_feat_loss += sum(torch.mean(torch.abs(f1 - f2)) for f1, f2 in zip(feats1[:-1], feats2[:-1])) / (#-1代表取出最后一组list
                        len(feats1) - 1)

        for i in range(2):
            feats1 = fmap_s_r[i]
            feats2 = fmap_s_g[i]
            # for j in range(7):
            #     feats1 = feats1_s_r[j]
            #     feats2 = feats2_s_g[j]
            assert len(feats1) == len(feats2)
            g_wave_loss += torch.mean(torch.relu(1 - feats2[-1]))
            g_feat_loss += sum(torch.mean(torch.abs(f1 - f2)) for f1, f2 in zip(feats1[:-1], feats2[:-1])) / (
                        len(feats1) - 1)

        self.log("g_wave_loss", g_wave_loss / 3, prog_bar=True)
        self.log("g_feat_loss", g_feat_loss / 3, prog_bar=True)

        g_rec_loss = self.rec_loss(output[:, 0, :], input[:, 0, :])
        self.log("g_rec_loss", g_rec_loss, prog_bar=True)

        g_feat_loss = g_feat_loss / 3
        g_adv_loss = (g_stft_loss + g_wave_loss) / 4
        g_loss = g_adv_loss + 100 * g_feat_loss + g_rec_loss
        self.log("q_loss", q_loss, prog_bar=True)
        self.log("g_loss", g_loss, prog_bar=True)

        self.manual_backward(g_loss + q_loss)
        optimizer_g.step()
        optimizer_g.zero_grad()
        self.untoggle_optimizer(optimizer_g)

        codes_entropy = self.quantizer.calc_entropy()
        self.log("codes_entropy", codes_entropy, prog_bar=True)

        # train discriminator
        self.toggle_optimizer(optimizer_d)
        output, _, _ = self.forward(input)
        y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = self.mpd(input, output)
        y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = self.msd(input, output)
        stft_out = self.stft_discriminator(input)
        d_stft_loss = torch.mean(torch.relu(1 - stft_out))
        stft_out = self.stft_discriminator(output)
        d_stft_loss += torch.mean(torch.relu(1 + stft_out))
        # d_stft_loss_mpd = 0
        # d_stft_loss_msd = 0
        # d_stft_loss_mpd_out = 0
        # d_stft_loss_msd_out = 0
        # for i in range(4):
        #     d_stft_loss_mpd += torch.mean(torch.relu(1 - y_df_hat_r[i]))
        # for i in range(2):
        #     d_stft_loss_msd += torch.mean(torch.relu(1 - y_ds_hat_r[i]))
        # d_stft_loss = d_stft_loss_mpd+d_stft_loss_msd
        # for i in range(4):
        #     d_stft_loss_mpd_out += torch.mean(torch.relu(1 + y_df_hat_g[i]))
        # for i in range(2):
        #     d_stft_loss_msd_out += torch.mean(torch.relu(1 + y_ds_hat_g[i]))
        # d_stft_loss +=d_stft_loss_mpd_out+d_stft_loss_msd_out

        d_wave_loss = 0

        # for i in range(3):
        #     feats = self.wave_discriminators[i](input)
        #     feat = feats[-1]
        #     d_wave_loss += torch.mean(
        #         torch.relu(1 - feats[-1]))  # feats为list7，分别是tensor:(32,4,,35710)(32,16,8928)...[-1]代表list的最后一个元素
        #     feats = self.wave_discriminators[i](output)
        #     d_wave_loss += torch.mean(torch.relu(1 + feats[-1]))

        for i in range(4):
            feats = fmap_f_r[i]
            feats_last = feats[-1]
            feats_last = feats_last.narrow(3, 1, 1)
            feats_last = torch.squeeze(feats_last, 3)
            d_wave_loss += torch.mean(
                torch.relu(1 - feats_last))  # feats为list7，分别是tensor:(32,4,,35710)(32,16,8928)...[-1]代表list的最后一个元素
            feats = fmap_f_g[i]
            feats_last = feats[-1]
            feats_last = feats_last.narrow(3, 1, 1)
            feats_last = torch.squeeze(feats_last, 3)
            d_wave_loss += torch.mean(torch.relu(1 + feats_last))

        for i in range(2):

            feats = fmap_s_r[i]
            # feats_last = feats[-1]
            # feats_last = feats_last.narrow(3, 1, 1)
            # feats_last = torch.squeeze(feats_last, 3)
            d_wave_loss += torch.mean(
                torch.relu(1 - feats[-1]))  # feats为list7，分别是tensor:(32,4,,35710)(32,16,8928)...[-1]代表list的最后一个元素
            feats = fmap_s_g[i]
            # feats_last = feats[-1]
            # feats_last = feats_last.narrow(3, 1, 1)
            # feats_last = torch.squeeze(feats_last, 3)
            d_wave_loss += torch.mean(torch.relu(1 + feats[-1]))

        self.log("d_stft_loss", d_stft_loss, prog_bar=True)
        self.log("d_wave_loss", d_wave_loss / 3, prog_bar=True)

        d_loss = (d_stft_loss + d_wave_loss) / 4
        self.log("d_loss", d_loss, prog_bar=True)

        self.manual_backward(d_loss)
        optimizer_d.step()
        optimizer_d.zero_grad()
        self.untoggle_optimizer(optimizer_d)

        num_replaced = self.quantizer.replace_vectors()
        self.log("num_replaced", float(num_replaced), prog_bar=True)

#↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑更换如上↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

    def train_dataloader(self):
        return self._make_dataloader(True)

    def _make_dataloader(self, train: bool):
        import torchaudio

        def collate(examples):
            return torch.stack(examples)

        class VoiceDataset(torch.utils.data.Dataset):
            def __init__(self, dataset, sample_rate, segment_length):
                self._dataset = dataset
                self._sample_rate = sample_rate
                self._segment_length = segment_length

            def __getitem__(self, index):
                import random
                x,sr, *_= self._dataset[index]
                # x, sr = load_wav(wav)
                x = torchaudio.functional.resample(x, sr, self._sample_rate)
                assert x.shape[0] == 1
                x = torch.squeeze(x)
                x *= 0.95 / torch.max(x)
                assert x.dim() == 1
                if x.shape[0] < self._segment_length:
                    x = F.pad(x, [0, self._segment_length - x.shape[0]], "constant")
                pos = random.randint(0, x.shape[0] - self._segment_length)
                x = x[pos:pos + self._segment_length]
                return x

            def __len__(self):
                return len(self._dataset)

        if self.hparams.dataset == 'yesno':
            ds = torchaudio.datasets.YESNO("./data", download=True)#Yesno：一个人在希伯来语中说是或否的 60 张唱片； 每个记录长 8 个字
        elif self.hparams.dataset == 'librispeech-dev':
            ds = torchaudio.datasets.LIBRISPEECH("./data", url="dev-clean")
        elif self.hparams.dataset == 'librispeech':#LibriSpeech：阅读英语语音的大型语料库（1000 小时）
            url = "train-clean-100" if train else "dev-clean"
            ds = torchaudio.datasets.LIBRISPEECH("./data", url=url)
        elif self.hparams.dataset == 'CIFAR10':
            ds = torchaudio.datasets.LIBRISPEECH("./data", download=False)
        else:
            raise ValueError()
        ds = VoiceDataset(ds, 8000, 35710 )#ds是子类，父类是VoiceDataset，父类是自定义的Dataset类


        loader = torch.utils.data.DataLoader(
            ds, batch_size=self.hparams['batch_size'], shuffle=True,
            collate_fn=collate)
        return loader

class KMeanCodebookInitCallback(pl.Callback):
    def on_fit_start(self, trainer, model):
        # https://arxiv.org/pdf/2107.03312.pdf
        # C. Residual Vector Quantizer
        # run the k-means
        # algorithm on the first training batch and use the learned
        # centroids as initialization
        batch = next(iter(model.train_dataloader()))
        input = batch[:, None, :].to(model.device)
        with torch.no_grad():
            x = torch.flatten(model.encoder(input))
            mean = torch.mean(x, axis=0)
            std = torch.std(x, axis=0)
            torch.nn.init.normal_(model.quantizer.weight, mean=mean, std=std)
        print(f"KMeanCodebookInitCallback {mean} {std}")

def train():
    parser = argparse.ArgumentParser()
    parser.add_argument("--device", type=str, default="cuda")
    a = parser.parse_args()
    model = StreamableModel(
        batch_size=16,
        sample_rate=8000,
        segment_length=35710,
        padding='same',
        dataset='CIFAR10',
        g_lr = 2e-5, # 张怀峰增加
        g_b1 = 0.8, # 张怀峰增加
        g_b2 = 0.99, # 张怀峰增加
        d_lr = 2e-5,  # 张怀峰增加
        d_b1 = 0.8,  # 张怀峰增加
        d_b2 = 0.99,# 张怀峰增加
    )
    model.to(a.device)

    trainer = pl.Trainer(
        max_epochs=150,
        log_every_n_steps=2,#更新n次网络权重后记录一次日志
        precision='16-mixed', 

        # logger=pl.loggers.CSVLogger("."),
        # logger=True,
        logger=pl.loggers.TensorBoardLogger("lightning_logs", name="20240126"),
        callbacks=[
            pl.callbacks.ModelCheckpoint(save_last=True, every_n_train_steps=500,save_top_k=-1),#按照步数保存checkpoint,-1表示保存所有ckpt
            KMeanCodebookInitCallback(),
        ],
    )

    # trainer.fit(model,None,None,None,'E:/000/soundstream-pytorch-main/lightning_logs/version_305/checkpoints/epoch=1-step=1600.ckpt')
    trainer.fit(model, None, None, None,None)
    return model

if __name__ == "__main__":
    train()
